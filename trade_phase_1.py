# -*- coding: utf-8 -*-
"""Trade Phase 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18u4GuzcieIme15wvihpS6X270xnmhGr8
"""

# ====================================================================================================
# Project Phase 1: Data Acquisition & Initial Exploration 6/19/2025
# Goal: Load raw data and get a first look at its structure and potential issues.
# ====================================================================================================
import pandas as pd
import numpy as np
from collections import deque # For FIFO calculation
import matplotlib.pyplot as plt # For visualizations
import seaborn as sns # For enhanced visualizations
# import plotly.express as px # Uncomment if you plan to use interactive Plotly charts later

# Step 2: Data Acquisition - Loading Raw Robinhood Data
# Robinhood exports are often CSVs. We anticipate potential issues like
# inconsistent rows (due to footers/metadata) which can cause ParserError.
try:
    # Use on_bad_lines='skip' to ignore rows with incorrect number of fields
    # For older pandas versions, you might need error_bad_lines=False and engine='python'
    trades_df = pd.read_csv('startto525.csv', on_bad_lines='skip')
    print("Data loaded successfully, problematic lines skipped.")
except FileNotFoundError:
    print("Error: Make sure the CSV file path is correct.")

# # Step 3: Initial Data Exploration & Understanding
# Thinking Process: After loading, we need to quickly inspect the data to
# understand its shape, column names, data types, and identify immediate
# data quality issues (e.g., NaNs, incorrect types for dates/numbers).

# print(trades_df.head()) # Commented out to reduce console output, but useful for quick check
print(trades_df.info())
print("\nRaw Column Names:", trades_df.columns.tolist())

# ====================================================================================================
# Project Phase 2: Data Cleaning & Preprocessing
# Goal: Transform raw, messy data into a clean, structured format suitable for analysis.
# This phase involves standardizing names, correcting types, and handling missing values.
# ====================================================================================================
# Step 4.1: Standardize Column Names
# Thinking Process: Raw column names often contain spaces or special characters
# which can make them cumbersome to work with in Python (e.g., using dot notation).
# Renaming them to snake_case makes the code cleaner and more readable.
new_column_names = {
    'Activity Date': 'activity_date',
    'Process Date': 'process_date',
    'Settle Date': 'settle_date',
    'Instrument': 'instrument',
    'Description': 'description',
    'Trans Code': 'trans_code',
    'Quantity': 'quantity',
    'Price': 'price',
    'Amount': 'amount'
}

trades_df = trades_df.rename(columns=new_column_names)
print("Column names standardized.")
print("New Column Names:", trades_df.columns.tolist())

# print("\nFirst 5 Rows with New Column Names:")
# print(trades_df.head())# Commented out to reduce console output, but useful for quick check

# Step 4.2: Convert Date Columns
print("\n--- Step 4.2: Converting Date Columns ---")
date_columns = ['activity_date', 'process_date', 'settle_date']
for col in date_columns:
    trades_df[col] = pd.to_datetime(trades_df[col], errors='coerce')
print("Date columns converted to datetime.")

# Step 4.3: Clean and Convert Numeric Columns (Price, Amount)
# Thinking Process: Price and Amount columns often contain currency symbols ($),
# commas (,) and parentheses for negative numbers '()'. These need to be removed
# and the values converted to a numeric type (float) for calculations.
print("\n--- Step 4.3: Cleaning and Converting Price & Amount ---")
def clean_currency(value):
    if pd.isna(value):
        return np.nan
    s_value = str(value).strip().replace('$', '').replace(',', '')
    if s_value.startswith('(') and s_value.endswith(')'):
        s_value = '-' + s_value[1:-1]
    try:
        return float(s_value)
    except ValueError:
        return np.nan

trades_df['price'] = trades_df['price'].apply(clean_currency)
trades_df['amount'] = trades_df['amount'].apply(clean_currency)
print("Price and Amount columns cleaned and converted to numeric.")

# Step 4.4: Investigate and Filter Non-Trade Transactions
# Thinking Process: We previously observed that 'quantity' and 'price' often have NaNs
# for non-trading activities (like dividends, interest, fees). Before focusing on trades,
# it's good practice to understand the nature of these NaNs and then filter the DataFrame.

print("\n--- Step 4.4: Investigating and Filtering Non-Trade Transactions ---")

# Identify rows where 'quantity' or 'price' are NaN in the original DataFrame
nan_quantity_price_rows = trades_df[trades_df['quantity'].isna() | trades_df['price'].isna()]

print("\nTransaction Codes with NaN in Quantity or Price:")
# Show the unique 'trans_code' values and their counts in these rows.
# This helps confirm that these NaNs are typically from non-trade activities.
print(nan_quantity_price_rows['trans_code'].value_counts())

# Filter the DataFrame to include only 'Buy' and 'Sell' transactions.
# This is our core dataset for trade performance analysis. '.copy()' is used
# to ensure we're working on a separate DataFrame and avoid SettingWithCopyWarning.
trade_codes = ['Buy', 'Sell']
trades_analysis_df = trades_df[trades_df['trans_code'].isin(trade_codes)].copy()

# (Optional: Store non-trading activities in a separate DataFrame for later analysis if desired)
# non_trades_df = trades_df[~trades_df['trans_code'].isin(trade_codes)].copy()
# print("\nNon-Trades Activities DataFrame Info (for your reference):")
# print(non_trades_df.info())

# Step 4.5: Final Numeric Type Conversion Check for 'quantity' and Drop Missing Critical Trade Data
# Thinking Process: Even after initial loading, the 'quantity' column might still contain
# non-numeric strings or have NaNs, especially after filtering. It must be a pure float
# for calculations. We then drop any rows where 'quantity' or 'price' are still NaN,
# as these are critical for defining a trade and cannot be calculated (e.g., a Buy without a quantity).
# it's good practice to understand the nature of these NaNs and then filter the DataFrame.
# Filter the DataFrame to include only 'Buy' and 'Sell' transactions.
# This is our core dataset for trade performance analysis. '.copy()' is used
# to ensure we're working on a separate DataFrame and avoid SettingWithCopyWarning.
trade_codes = ['Buy', 'Sell']
trades_analysis_df = trades_df[trades_df['trans_code'].isin(trade_codes)].copy()

# (Optional: Store non-trading activities in a separate DataFrame for later analysis if desired)
# non_trades_df = trades_df[~trades_df['trans_code'].isin(trade_codes)].copy()
# print("\nNon-Trades Activities DataFrame Info (for your reference):")
# print(non_trades_df.info())

print("\n--- Step 4.5: Final Numeric Type & NaN Check for Quantity/Price in Trades ---")

# Ensure 'quantity' is truly numeric in trades_analysis_df
# First, convert to string to handle various formats, then remove commas.
trades_analysis_df['quantity'] = trades_analysis_df['quantity'].astype(str).str.replace(',', '', regex=False)
# Now, convert to numeric, coercing any non-convertible values to NaN.
trades_analysis_df['quantity'] = pd.to_numeric(trades_analysis_df['quantity'], errors='coerce')

# Drop any rows where 'quantity' or 'price' are NaN after these conversions.
# For 'Buy'/'Sell' trades, both quantity and price are essential.
trades_analysis_df.dropna(subset=['quantity', 'price'], inplace=True)
# note - I want a comment that show how many rows drop
print("\n--- Final Trades Analysis DataFrame Info ---")
print(trades_analysis_df.info())
print("\n--- First 5 Rows of Final Trades Analysis DataFrame ---")
print(trades_analysis_df.head())
# Identify Unique Instruments (Tickers)
# Thinking Process: Getting a list of unique instruments is useful for understanding
# the breadth of trading activity and for later filtering/analysis by specific stocks.
unique_instruments = trades_analysis_df['instrument'].unique().tolist()
print("\nUnique Instruments Traded:")
print(unique_instruments)
# note - i want to know total instruments number

# ====================================================================================================
# Project Phase 3: Advanced Feature Engineering (FIFO Profit/Loss Calculation)
# Goal: Calculate the realized profit/loss for each closed position, matching buy and sell orders.
# This is the core analytical challenge in trading data.
# ====================================================================================================

print("\n--- Phase 3: Advanced Feature Engineering (FIFO P/L) ---")

# Step 4.6: Implement FIFO Profit/Loss Calculation
# Thinking Process: Robinhood provides individual buy/sell entries, not closed P/L per trade.
# To calculate realized profit/loss, we need to match sells against buys.
# FIFO (First-In, First-Out) is a common accounting method: the first shares bought are
# considered the first shares sold. This involves iterating through transactions for each
# instrument, maintaining a queue of open buy lots, and processing sells against them.

closed_positions = [] # List to store the details of each closed position

# Group the DataFrame by 'instrument' (stock ticker) and sort by date for chronological processing.
# 'process_date' and 'settle_date' are added as secondary sort keys for robust chronological order
# in case 'activity_date' has ties, which is crucial for correct FIFO logic.
grouped_by_instrument = trades_analysis_df.sort_values(by=['activity_date', 'process_date', 'settle_date']).groupby('instrument')

for instrument_name, group in grouped_by_instrument:
    # 'buy_lots' deque (double-ended queue) acts as our FIFO stack.
    # Each item stores the quantity, price, and purchase date of a bought lot.
    buy_lots = deque()

    for index, row in group.iterrows():
        trans_code = row['trans_code']
        quantity = row['quantity']
        price = row['price']
        activity_date = row['activity_date'] # Date of the current buy/sell transaction

        if trans_code == 'Buy':
            # When a 'Buy' occurs, add the details of this new lot to the end of the queue.
            buy_lots.append({'quantity': quantity, 'price': price, 'date': activity_date})
        elif trans_code == 'Sell':
            sold_quantity_remaining = quantity # The total quantity from the current sell order
            realized_profit_loss = 0
            cost_basis_for_this_sale = 0
            bought_dates_for_sale = [] # To track buy dates for holding period calculation

            # Process the sell order by consuming shares from the oldest buy lots (FIFO)
            # Continue as long as there are shares to sell and available buy lots.
            while sold_quantity_remaining > 0 and buy_lots:
                oldest_lot = buy_lots[0] # Get the oldest buy lot (front of the queue)
                lot_quantity = oldest_lot['quantity']
                lot_price = oldest_lot['price']
                lot_date = oldest_lot['date']

                # Determine how many shares from the current oldest lot will be sold
                quantity_to_sell_from_lot = min(sold_quantity_remaining, lot_quantity)

                # Calculate P/L and cost basis for the shares sold from this specific lot
                realized_profit_loss += (price - lot_price) * quantity_to_sell_from_lot
                cost_basis_for_this_sale += lot_price * quantity_to_sell_from_lot

                # Update remaining quantities
                sold_quantity_remaining -= quantity_to_sell_from_lot # Decrease shares still needed for current sell order
                oldest_lot['quantity'] -= quantity_to_sell_from_lot # Decrease shares remaining in the current buy lot

                # Record the buy date of this lot for holding period calculation later
                bought_dates_for_sale.append(lot_date)

                # If the oldest buy lot is fully consumed, remove it from the queue
                if oldest_lot['quantity'] == 0:
                    buy_lots.popleft()

            # After processing a sell order, record the details if any shares were actually matched and sold.
            # 'quantity != sold_quantity_remaining' means some shares were successfully sold and accounted for.
            if quantity != sold_quantity_remaining:
                # Calculate the holding period for this closed sale.
                # If multiple buy dates contributed, use the earliest buy date to be conservative.
                if bought_dates_for_sale:
                    earliest_buy_date = min(bought_dates_for_sale)
                    holding_period = (activity_date - earliest_buy_date).days
                else:
                    # This case should ideally not happen if data is perfectly matched, but included as a fallback.
                    holding_period = np.nan

                closed_positions.append({
                    'instrument': instrument_name,
                    'sell_date': activity_date,
                    'sell_price': price,
                    'sold_quantity_total': quantity, # Total quantity from this specific sell order
                    'realized_profit_loss': realized_profit_loss,
                    'cost_basis_for_sale': cost_basis_for_this_sale,
                    'holding_period_days': holding_period
                })

# Convert the list of closed positions to a DataFrame
closed_positions_df = pd.DataFrame(closed_positions)

# print("\n--- Closed Positions DataFrame Info (Step 4.6) ---")
# print(closed_positions_df.info())
# print("\n--- First 10 Rows of Closed Positions (Step 4.6) ---")
# print(closed_positions_df.head(10))

# IMPORTANT NOTE ON UNREALIZED POSITIONS:
# After this FIFO loop, any remaining items in `buy_lots` (per instrument, if not empty)
# would represent your current *unrealized* holdings. This project focuses on realized
# P/L, but this is a logical extension for a full portfolio analysis.

# ====================================================================================================
# Project Phase 4: Analysis & Aggregation
# Goal: Calculate key performance metrics from the cleaned and engineered data.
# ====================================================================================================

print("\n--- Phase 4: Analysis & Aggregation ---")

# Step 5.1: Calculate Overall Performance Metrics
# Thinking Process: These metrics provide a high-level summary of trading success.
# They directly address the "final outcome" question.
total_realized_pl = closed_positions_df['realized_profit_loss'].sum()
print(f"\nTotal Realized Profit/Loss: ${total_realized_pl:,.2f}")

total_trades = len(closed_positions_df)
print(f"Total Number of Closed Trades: {total_trades}")

average_pl_per_trade = closed_positions_df['realized_profit_loss'].mean()
print(f"Average Profit/Loss per Trade: ${average_pl_per_trade:,.2f}")

profitable_trades = closed_positions_df[closed_positions_df['realized_profit_loss'] > 0]
losing_trades = closed_positions_df[closed_positions_df['realized_profit_loss'] <= 0] # Include zero P/L as non-profitable

num_profitable_trades = len(profitable_trades)
num_losing_trades = len(losing_trades)
print(f"Number of Profitable Trades: {num_profitable_trades}")
print(f"Number of Losing Trades: {num_losing_trades}")

win_rate = (num_profitable_trades / total_trades) * 100 if total_trades > 0 else 0
print(f"Win Rate: {win_rate:.2f}%")

average_holding_period_overall = closed_positions_df['holding_period_days'].mean()
average_holding_period_profitable = profitable_trades['holding_period_days'].mean()
average_holding_period_losing = losing_trades['holding_period_days'].mean()

print(f"Average Holding Period (Overall): {average_holding_period_overall:.2f} days")
print(f"Average Holding Period (Profitable Trades): {average_holding_period_profitable:.2f} days")
print(f"Average Holding Period (Losing Trades): {average_holding_period_losing:.2f} days")

# Step 5.2: Analyze Performance by Instrument
# Group by instrument and calculate aggregated metrics
# strengths and weaknesses, and which assets contributed most to gains/losses.
instrument_performance = closed_positions_df.groupby('instrument').agg(
    total_pl=('realized_profit_loss', 'sum'),
    num_trades=('instrument', 'count'),
    avg_holding_period=('holding_period_days', 'mean')
).reset_index()

# Sort by total profit/loss to see top and bottom performers
top_10_profitable = instrument_performance.sort_values(by='total_pl', ascending=False).head(10)
bottom_10_losing = instrument_performance.sort_values(by='total_pl', ascending=True).head(10)

print("\n--- Top 10 Most Profitable Instruments ---")
print(top_10_profitable.round(2))

print("\n--- Top 10 Least Profitable (or Most Losing) Instruments ---")
print(bottom_10_losing.round(2))

# You can also look at instruments with the most trades
top_10_most_traded = instrument_performance.sort_values(by='num_trades', ascending=False).head(10)
print("\n--- Top 10 Most Frequently Traded Instruments ---")
print(top_10_most_traded.round(2))

# Step 5.3: Analyze Performance Over Time (Cumulative P/L)
# Thinking Process: Understanding the trend of profit accumulation over time
# provides context to the overall profit and highlights periods of growth or drawdown.
# Ensure closed_positions_df is sorted by sell_date for cumulative calculation
closed_positions_df_sorted = closed_positions_df.sort_values(by='sell_date')

# Calculate Cumulative Realized Profit/Loss
closed_positions_df_sorted['cumulative_pl'] = closed_positions_df_sorted['realized_profit_loss'].cumsum()

print("\n--- First 10 Rows with Cumulative P/L ---")
print(closed_positions_df_sorted[['sell_date', 'realized_profit_loss', 'cumulative_pl']].head(10))

print("\n--- Last 10 Rows with Cumulative P/L (to see final trend) ---")
print(closed_positions_df_sorted[['sell_date', 'realized_profit_loss', 'cumulative_pl']].tail(10))

# Optionally, aggregate by month or year for a broader view
monthly_pl = closed_positions_df_sorted.set_index('sell_date').resample('M')['realized_profit_loss'].sum().reset_index()
monthly_pl['cumulative_pl'] = monthly_pl['realized_profit_loss'].cumsum()

print("\n--- Monthly Cumulative P/L (First 5 Rows) ---")
print(monthly_pl.head())
print("\n--- Monthly Cumulative P/L (Last 5 Rows) ---")
print(monthly_pl.tail())

# ====================================================================================================
# Project Phase 5: Visualization
# Goal: Create compelling visual representations of the key insights derived from the analysis.
# ====================================================================================================

print("\n--- Phase 5: Visualizations ---")

import matplotlib.pyplot as plt
import seaborn as sns

# Set a style for better aesthetics
sns.set_style("whitegrid")
plt.figure(figsize=(12, 6))

# --- 1. Cumulative Realized Profit/Loss Over Time ---
# Thinking Process: This plot is central to telling the story of profitability over time.
# It clearly shows growth, drawdowns, and recovery.
plt.figure(figsize=(12, 6))
plt.plot(closed_positions_df_sorted['sell_date'], closed_positions_df_sorted['cumulative_pl'], label='Daily Cumulative P/L')
plt.plot(monthly_pl['sell_date'], monthly_pl['cumulative_pl'], marker='o', linestyle='--', color='orange', label='Monthly Cumulative P/L') # Add monthly for smoother trend
plt.title('Cumulative Realized Profit/Loss Over Time')
plt.xlabel('Date')
plt.ylabel('Cumulative Profit/Loss ($)')
plt.axhline(0, color='red', linestyle='--', linewidth=0.8, label='Break-Even Line') # Add a break-even line
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# --- 2. Top 10 Most Profitable Instruments (Bar Chart) ---
# Thinking Process: Visualizing top performers quickly shows where the most gains came fro
plt.figure(figsize=(12, 7))
sns.barplot(x='total_pl', y='instrument', data=top_10_profitable, palette='viridis')
plt.title('Top 10 Most Profitable Instruments')
plt.xlabel('Total Realized P/L ($)')
plt.ylabel('Instrument')
plt.tight_layout()
plt.show()

# --- 3. Top 10 Least Profitable (Most Losing) Instruments (Bar Chart) ---
# Thinking Process: Equally important as winners, this highlights areas of significant loss.
plt.figure(figsize=(12, 7))
# Use a diverging palette or specify color for negative values
sns.barplot(x='total_pl', y='instrument', data=bottom_10_losing, palette='rocket')
plt.title('Top 10 Least Profitable Instruments')
plt.xlabel('Total Realized P/L ($)')
plt.ylabel('Instrument')
plt.tight_layout()
plt.show()

# --- 4. Distribution of Holding Periods (Histogram) ---
# Thinking Process: This histogram gives a general overview of how long trades are typically held.
# It helps categorize trading style (e.g., short-term vs. long-term bias).
plt.figure(figsize=(10, 6))
sns.histplot(closed_positions_df['holding_period_days'], bins=50, kde=True)
plt.title('Distribution of Holding Periods (Days)')
plt.xlabel('Holding Period (Days)')
plt.ylabel('Number of Trades')
plt.tight_layout()
plt.show()

# --- 5. Holding Period Comparison (Profitable vs. Losing Trades) ---
# Thinking Process: This is a critical behavioral insight. Comparing holding periods
# for profitable vs. losing trades often reveals patterns in trading psychology.
plt.figure(figsize=(10, 6))
sns.histplot(profitable_trades['holding_period_days'], color='green', label='Profitable Trades', kde=True, stat='density', alpha=0.5, bins=30)
sns.histplot(losing_trades['holding_period_days'], color='red', label='Losing Trades', kde=True, stat='density', alpha=0.5, bins=30)
plt.title('Holding Period Distribution: Profitable vs. Losing Trades')
plt.xlabel('Holding Period (Days)')
plt.ylabel('Density')
plt.legend()
plt.tight_layout()
plt.show()

# --- Optional: P/L Distribution (Histogram) ---
# Thinking Process: This plot shows the frequency of various profit/loss amounts per trade.
# It typically has a tall peak near zero and tails for larger wins/losses.
plt.figure(figsize=(10, 6))
sns.histplot(closed_positions_df['realized_profit_loss'], bins=50, kde=True)
plt.title('Distribution of Realized Profit/Loss per Trade')
plt.xlabel('Realized Profit/Loss ($)')
plt.ylabel('Number of Trades')
plt.tight_layout()
plt.show()

# ====================================================================================================
# Project Phase 6: Data Export for App Development
# Goal: Save the cleaned and analyzed DataFrames for easy loading into a Streamlit/Dash application.
# This avoids re-running all heavy calculations every time the app starts.
# ====================================================================================================

print("\n--- Phase 6: Data Export for App Development ---")
# Thinking Process: For a standalone application (like Streamlit), it's more efficient
# to load pre-processed data rather than re-running the entire cleaning and FIFO calculation.
# Exporting these key DataFrames to CSVs makes them readily available for the app.
try:
    closed_positions_df.to_csv('closed_positions.csv', index=False)
    monthly_pl.to_csv('monthly_pl.csv', index=False)
    instrument_performance.to_csv('instrument_performance.csv', index=False)
    print("\nProcessed DataFrames saved successfully for Streamlit app development:")
    print(" - closed_positions.csv")
    print(" - monthly_pl.csv")
    print(" - instrument_performance.csv")
except Exception as e:
    print(f"Error saving dataframes: {e}")

print("\n--- Analysis Script Complete ---")
#closed_positions_df.to_csv('closed_positions.csv', index=False)
#monthly_pl.to_csv('monthly_pl.csv', index=False)
#instrument_performance.to_csv('instrument_performance.csv', index=False)